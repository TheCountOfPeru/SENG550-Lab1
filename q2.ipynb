{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "import sys\n",
    "import string\n",
    "\n",
    "for line in sys.stdin: # reads from stdin\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "\n",
    "    for word in words: # writes to stdout\n",
    "        word = word.translate(str.maketrans('','',string.punctuation))\n",
    "        if word.isnumeric() is False:\n",
    "            print(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "\n",
    "import sys\n",
    "import random\n",
    "\n",
    "counts = dict()\n",
    "bigram = \"\"\n",
    "k = 0\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    if k == 0:\n",
    "        bigram = line\n",
    "        k = k+1\n",
    "    else:\n",
    "        bigram = bigram + \" \" + line\n",
    "        if bigram in counts:\n",
    "            counts[bigram] += 1\n",
    "        else:\n",
    "            counts[bigram] = 1\n",
    "        bigram = line\n",
    "\n",
    "for key in counts:\n",
    "    print(key +\"\\t\"+ str(counts[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/shakespeare.txt | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /outputs/result2\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R /outputs/result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-16 23:03:24,403 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/training/mapper.py, /training/reducer.py, /tmp/hadoop-unjar4621688359438258911/] [] /tmp/streamjob600543118665798668.jar tmpDir=null\n",
      "2020-10-16 23:03:27,754 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.4:8032\n",
      "2020-10-16 23:03:28,398 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.5:10200\n",
      "2020-10-16 23:03:28,442 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.4:8032\n",
      "2020-10-16 23:03:28,442 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.5:10200\n",
      "2020-10-16 23:03:28,981 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1602883786030_0002\n",
      "2020-10-16 23:03:29,757 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-16 23:03:30,345 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-16 23:03:30,861 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-16 23:03:31,267 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2020-10-16 23:03:31,724 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-16 23:03:32,517 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-16 23:03:32,612 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2020-10-16 23:03:33,047 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-16 23:03:33,568 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1602883786030_0002\n",
      "2020-10-16 23:03:33,568 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-10-16 23:03:34,000 INFO conf.Configuration: resource-types.xml not found\n",
      "2020-10-16 23:03:34,000 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2020-10-16 23:03:35,331 INFO impl.YarnClientImpl: Submitted application application_1602883786030_0002\n",
      "2020-10-16 23:03:35,541 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1602883786030_0002/\n",
      "2020-10-16 23:03:35,544 INFO mapreduce.Job: Running job: job_1602883786030_0002\n",
      "2020-10-16 23:03:52,524 INFO mapreduce.Job: Job job_1602883786030_0002 running in uber mode : false\n",
      "2020-10-16 23:03:52,526 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-10-16 23:05:23,539 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "2020-10-16 23:05:29,595 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2020-10-16 23:05:35,320 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2020-10-16 23:07:27,043 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2020-10-16 23:07:38,758 INFO mapreduce.Job: Job job_1602883786030_0002 completed successfully\n",
      "2020-10-16 23:07:44,236 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=173799\n",
      "\t\tFILE: Number of bytes written=1107187\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5454669\n",
      "\t\tHDFS: Number of bytes written=816037\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=769652\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=770392\n",
      "\t\tTotal time spent by all map tasks (ms)=192413\n",
      "\t\tTotal time spent by all reduce tasks (ms)=96299\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=192413\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=96299\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=788123648\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=788881408\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=122395\n",
      "\t\tMap output records=882855\n",
      "\t\tMap output bytes=5462138\n",
      "\t\tMap output materialized bytes=235049\n",
      "\t\tInput split bytes=200\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=27974\n",
      "\t\tReduce shuffle bytes=235049\n",
      "\t\tReduce input records=882855\n",
      "\t\tReduce output records=43867\n",
      "\t\tSpilled Records=1765710\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1670\n",
      "\t\tCPU time spent (ms)=13090\n",
      "\t\tPhysical memory (bytes) snapshot=529285120\n",
      "\t\tVirtual memory (bytes) snapshot=18441199616\n",
      "\t\tTotal committed heap usage (bytes)=338690048\n",
      "\t\tPeak Map Physical memory (bytes)=215351296\n",
      "\t\tPeak Map Virtual memory (bytes)=5067759616\n",
      "\t\tPeak Reduce Physical memory (bytes)=116682752\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8377499648\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5454469\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=816037\n",
      "2020-10-16 23:07:44,343 INFO streaming.StreamJob: Output directory: /outputs/result2\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "    -file $PWD/mapper.py\\\n",
    "    -file $PWD/reducer.py\\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /inputs/data \\\n",
    "    -output /outputs/result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /outputs/result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-16 23:10:17,284 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "youngly youngs\t1\n",
      "youngs youngst\t1\n",
      "youngst younker\t1\n",
      "younker younker\t2\n",
      "younker youoften\t1\n",
      "youoften youpray\t1\n",
      "youpray your\t1\n",
      "your your\t6654\n",
      "your youre\t1\n",
      "youre youre\t25\n",
      "youre yours\t1\n",
      "yours yours\t254\n",
      "yours yourself\t1\n",
      "yourself yourself\t279\n",
      "yourself yourselves\t1\n",
      "yourselves yourselves\t72\n",
      "yourselves yoursnot\t1\n",
      "yoursnot youst\t1\n",
      "youst youth\t1\n",
      "youth youth\t276\n",
      "youth youthat\t1\n",
      "youthat youthat\t1\n",
      "youthat youthful\t1\n",
      "youthful youthful\t30\n",
      "youthful youths\t1\n",
      "youths youths\t9\n",
      "youths youtli\t1\n",
      "youtli youwell\t1\n",
      "youwell youwondrous\t1\n",
      "youwondrous youyou\t1\n",
      "youyou zanies\t1\n",
      "zanies zany\t1\n",
      "zany zeal\t1\n",
      "zeal zeal\t32\n",
      "zeal zealous\t1\n",
      "zealous zealous\t5\n",
      "zealous zeals\t1\n",
      "zeals zed\t1\n",
      "zed zenelophon\t1\n",
      "zenelophon zenith\t1\n",
      "zenith zephyrs\t1\n",
      "zephyrs zir\t1\n",
      "zir zir\t1\n",
      "zir zo\t1\n",
      "zo zodiac\t1\n",
      "zodiac zodiacs\t1\n",
      "zodiacs zone\t1\n",
      "zone zounds\t1\n",
      "zounds zounds\t23\n",
      "zounds zwaggerd\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/result2/part-00000 | tail -n 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
